{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretizing continuous observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 5\n",
    "eta = 0.85\n",
    "gamma =  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/F77925B/workspace/manuel/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "pos  | vel\n",
      "[-0.51282174  0.        ]\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "\n",
    "print(\"pos  | vel\")\n",
    "print(obs)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36  0.028]\n"
     ]
    }
   ],
   "source": [
    "# 2d cell defining the \n",
    "env_dx = (env.observation_space.high - env.observation_space.low) / n_states\n",
    "print(env_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0]) / env_dx[0])\n",
    "    b = int((obs[1] - env_low[1]) / env_dx[1])\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_to_state(env, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((n_states, n_states, 3))\n",
    "\n",
    "for i in range(2000):\n",
    "    a, b  = obs_to_state(env, obs)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    a_, b_  = obs_to_state(env, obs)\n",
    "#     print(obs, ' -> ', a, b)\n",
    "    q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma * np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "    \n",
    "#     print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -7.64760311  -5.68290136  -6.68325047]\n",
      "  [-10.37512796  -8.26542852  -9.53670722]\n",
      "  [ -9.91556748  -7.44206942  -8.91819669]\n",
      "  [ -7.34653645  -8.07541127  -7.13925898]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[ -9.33479061  -8.75446305  -8.33305322]\n",
      "  [ -8.57523225  -7.65294764  -8.02663301]\n",
      "  [  0.           0.           0.        ]\n",
      "  [ -8.76335359  -7.44672612  -8.3500615 ]\n",
      "  [ -8.43718936 -10.9596911  -10.0315854 ]]\n",
      "\n",
      " [[ -7.02023007  -9.17225483  -7.00438527]\n",
      "  [ -9.25539902  -9.81208571  -9.81199362]\n",
      "  [  0.           0.           0.        ]\n",
      "  [ -8.03003639  -8.63535413  -8.17998616]\n",
      "  [ -6.55745563  -9.83120442  -8.29369078]]\n",
      "\n",
      " [[  0.           0.           0.        ]\n",
      "  [ -8.63074256  -8.7650084   -8.73230546]\n",
      "  [-10.20181256  -8.84347637 -10.9067151 ]\n",
      "  [ -9.2433385   -9.60669788  -8.71654955]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [ -2.65630241  -0.996625     0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.33479061, -8.75446305, -8.33305322])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[a, b, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_episodes = 100\n",
    "num_timesteps = 1000\n",
    "\n",
    "learning_rate = 0.85\n",
    "\n",
    "max_eps = 1.0\n",
    "min_eps = 0.01\n",
    "eps_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epsilon decay\n",
    "ep = 0\n",
    "min_eps + (max_eps - min_eps) * np.exp(-eps_decay_rate * ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init q-table\n",
    "q_table = np.zeros((n_states, n_states, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: -200.0\n",
      "[[[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [  -2.83250096   -2.8303605   -23.09629032]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [ -74.30328937  -74.29348477  -75.42267396]\n",
      "  [ -96.58623106  -96.43869633  -97.41921928]\n",
      "  [ -71.93514928  -69.37793539  -69.44103581]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [ -73.88332869  -65.05378718 -100.55899245]\n",
      "  [ -96.83674814  -96.86718647  -94.09283058]\n",
      "  [ -75.9253324   -68.28018356  -68.82529825]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    obs = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    epsilon = 1.0\n",
    "    \n",
    "    for step in range(num_timesteps):\n",
    "        \n",
    "        # discretize observation to space\n",
    "        a, b = obs_to_state(env, obs)\n",
    "        \n",
    "        # Choose an action a in the current world state (s)\n",
    "        \n",
    "        ## If random number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[a, b ,:])\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        new_a, new_b = obs_to_state(env, new_obs)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        q_table[a, b, action] = q_table[a, b, action] + learning_rate * (reward + gamma * np.max(q_table[new_a, new_b, :]) - q_table[a, b, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        obs = new_obs\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_eps + (max_eps - min_eps) * np.exp(-eps_decay_rate * episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/num_episodes))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
