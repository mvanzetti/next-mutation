{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretizing continuous observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 5\n",
    "eta = 0.85\n",
    "gamma =  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/F77925B/workspace/manuel/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "pos  | vel\n",
      "[-0.58819075  0.        ]\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "\n",
    "print(\"pos  | vel\")\n",
    "print(obs)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36  0.028]\n"
     ]
    }
   ],
   "source": [
    "# 2d cell defining the \n",
    "env_dx = (env.observation_space.high - env.observation_space.low) / n_states\n",
    "print(env_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0]) / env_dx[0])\n",
    "    b = int((obs[1] - env_low[1]) / env_dx[1])\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_to_state(env, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((n_states, n_states, 3))\n",
    "\n",
    "for i in range(2000):\n",
    "    a, b  = obs_to_state(env, obs)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    a_, b_  = obs_to_state(env, obs)\n",
    "#     print(obs, ' -> ', a, b)\n",
    "    q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma * np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "    \n",
    "#     print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.           0.           0.        ]\n",
      "  [ -1.9683875   -0.9775      -3.61980517]\n",
      "  [ -7.30099799  -5.58831711  -5.58800825]\n",
      "  [-16.22372493  -2.06732036 -16.2677718 ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.        ]\n",
      "  [ -8.40165625  -7.57127839  -9.56525879]\n",
      "  [ -7.43496392  -7.27704269  -9.2092471 ]\n",
      "  [-10.91216559 -11.17033173  -9.17792242]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.        ]\n",
      "  [ -6.29516595  -6.64348737  -5.31207127]\n",
      "  [ -6.09274058  -9.41165055  -8.71150261]\n",
      "  [ -7.87597554  -8.61575787  -9.46924674]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [ -7.25893916  -1.00131879  -1.84711986]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.        ,   0.        ,   0.        ],\n",
       "        [ -1.9683875 ,  -0.9775    ,  -3.61980517],\n",
       "        [ -7.30099799,  -5.58831711,  -5.58800825],\n",
       "        [-16.22372493,  -2.06732036, -16.2677718 ],\n",
       "        [  0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.        ,   0.        ,   0.        ],\n",
       "        [ -8.40165625,  -7.57127839,  -9.56525879],\n",
       "        [ -7.43496392,  -7.27704269,  -9.2092471 ],\n",
       "        [-10.91216559, -11.17033173,  -9.17792242],\n",
       "        [  0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.        ,   0.        ,   0.        ],\n",
       "        [ -6.29516595,  -6.64348737,  -5.31207127],\n",
       "        [ -6.09274058,  -9.41165055,  -8.71150261],\n",
       "        [ -7.87597554,  -8.61575787,  -9.46924674],\n",
       "        [  0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ],\n",
       "        [ -7.25893916,  -1.00131879,  -1.84711986],\n",
       "        [  0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.91216559, -11.17033173,  -9.17792242])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[a, b, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "n_actions = 3\n",
    "n_observation_space = 2\n",
    "shape = (n_states, n_states, n_actions)\n",
    "q_matrix = np.random.rand(n_states, n_states, n_actions)\n",
    "\n",
    "print(q_matrix.shape)\n",
    "assert shape == q_matrix.shape\n",
    "\n",
    "observation = np.random.rand(n_observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "a,b = obs_to_state(env, observation)\n",
    "print(a,b)\n",
    "state = [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7791643 , 0.29845775, 0.08873795],\n",
       "        [0.27661256, 0.44125511, 0.53955589],\n",
       "        [0.78826117, 0.64809392, 0.49591479],\n",
       "        [0.87517358, 0.27103591, 0.77428262],\n",
       "        [0.6654274 , 0.79757156, 0.88553725]],\n",
       "\n",
       "       [[0.5327819 , 0.41758545, 0.38779637],\n",
       "        [0.74443818, 0.14901641, 0.99723817],\n",
       "        [0.5390338 , 0.48326593, 0.91956903],\n",
       "        [0.60856221, 0.92970324, 0.44436295],\n",
       "        [0.92991791, 0.70910561, 0.93624505]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_matrix[state, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_episodes = 100\n",
    "num_timesteps = 1000\n",
    "\n",
    "learning_rate = 0.85\n",
    "\n",
    "max_eps = 1.0\n",
    "min_eps = 0.01\n",
    "eps_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epsilon decay\n",
    "ep = 0\n",
    "min_eps + (max_eps - min_eps) * np.exp(-eps_decay_rate * ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init q-table\n",
    "q_table = np.zeros((n_states, n_states, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: -200.0\n",
      "[[[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [  -2.83250096   -2.8303605   -23.09629032]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [ -74.30328937  -74.29348477  -75.42267396]\n",
      "  [ -96.58623106  -96.43869633  -97.41921928]\n",
      "  [ -71.93514928  -69.37793539  -69.44103581]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [ -73.88332869  -65.05378718 -100.55899245]\n",
      "  [ -96.83674814  -96.86718647  -94.09283058]\n",
      "  [ -75.9253324   -68.28018356  -68.82529825]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    obs = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    epsilon = 1.0\n",
    "    \n",
    "    for step in range(num_timesteps):\n",
    "        \n",
    "        # discretize observation to space\n",
    "        a, b = obs_to_state(env, obs)\n",
    "        \n",
    "        # Choose an action a in the current world state (s)\n",
    "        \n",
    "        ## If random number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[a, b ,:])\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        new_a, new_b = obs_to_state(env, new_obs)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        q_table[a, b, action] = q_table[a, b, action] + learning_rate * (reward + gamma * np.max(q_table[new_a, new_b, :]) - q_table[a, b, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        obs = new_obs\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_eps + (max_eps - min_eps) * np.exp(-eps_decay_rate * episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/num_episodes))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
